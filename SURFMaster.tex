\documentclass{article}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usetikzlibrary{timeline}
\usepackage{booktabs}
\usepackage{float}
\restylefloat{table}
\graphicspath{{images/}}
\usepackage[margin={1.5cm,2cm}]{geometry}
\usepackage{multicol}
\setlength\columnsep{1.5cm}
\usepackage{tabto}
\usepackage{pdflscape}
\usepackage{graphicx}
\usepackage{array}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{charter}
\usepackage{environ}
\usepackage{tikz}
\usetikzlibrary{calc,matrix}

% code by Andrew:
% http://tex.stackexchange.com/a/28452/13304
\makeatletter
\let\matamp=&
\catcode`\&=13
\makeatletter
\def&{\iftikz@is@matrix
  \pgfmatrixnextcell
  \else
  \matamp
  \fi}
\makeatother

\newcounter{lines}
\def\endlr{\stepcounter{lines}\\}

\newcounter{vtml}
\setcounter{vtml}{0}

\newif\ifvtimelinetitle
\newif\ifvtimebottomline
\tikzset{description/.style={
  column 2/.append style={#1}
 },
 timeline color/.store in=\vtmlcolor,
 timeline color=red!80!black,
 timeline color st/.style={fill=\vtmlcolor,draw=\vtmlcolor},
 use timeline header/.is if=vtimelinetitle,
 use timeline header=false,
 add bottom line/.is if=vtimebottomline,
 add bottom line=false,
 timeline title/.store in=\vtimelinetitle,
 timeline title={},
 line offset/.store in=\lineoffset,
 line offset=4pt,
}

\NewEnviron{vtimeline}[1][]{%
\setcounter{lines}{1}%
\stepcounter{vtml}%
\begin{tikzpicture}[column 1/.style={anchor=east},
 column 2/.style={anchor=west},
 text depth=0pt,text height=1ex,
 row sep=1ex,
 column sep=1em,
 #1
]
\matrix(vtimeline\thevtml)[matrix of nodes]{\BODY};
\pgfmathtruncatemacro\endmtx{\thelines-1}
\path[timeline color st] 
($(vtimeline\thevtml-1-1.north east)!0.5!(vtimeline\thevtml-1-2.north west)$)--
($(vtimeline\thevtml-\endmtx-1.south east)!0.5!(vtimeline\thevtml-\endmtx-2.south west)$);
\foreach \x in {1,...,\endmtx}{
 \node[circle,timeline color st, inner sep=0.15pt, draw=white, thick] 
 (vtimeline\thevtml-c-\x) at 
 ($(vtimeline\thevtml-\x-1.east)!0.5!(vtimeline\thevtml-\x-2.west)$){};
 \draw[timeline color st](vtimeline\thevtml-c-\x.west)--++(-3pt,0);
 }
 \ifvtimelinetitle%
  \draw[timeline color st]([yshift=\lineoffset]vtimeline\thevtml.north west)--
  ([yshift=\lineoffset]vtimeline\thevtml.north east);
  \node[anchor=west,yshift=16pt,font=\large]
   at (vtimeline\thevtml-1-1.north west) 
   {\textsc{Timeline \thevtml}: \textit{\vtimelinetitle}};
 \else%
  \relax%
 \fi%
 \ifvtimebottomline%
   \draw[timeline color st]([yshift=-\lineoffset]vtimeline\thevtml.south west)--
  ([yshift=-\lineoffset]vtimeline\thevtml.south east);
 \else%
   \relax%
 \fi%
\end{tikzpicture}
}

\begin{document}
\begin{titlepage}
	\centering
	\begin{figure}[H]
	\centering
	%\includegraphics[scale=0.5]{logo_nasa_trio_black@2x.png}
	\end{figure}
	\vspace{2cm}
	{\scshape\LARGE NCRA-TIFR Project Proposal \par}
	\vspace{2cm}
	{\huge\bfseries \par}
	\center\includegraphics[scale=0.5]{NCRATIFR.jpg}
	
	\vspace{2cm}
	{\Large\itshape Archit Sakhadeo\par}
	{\Large\itshape Rathin Desai\par}
	{\Large\itshape Shadab Shaikh\par}
	{\Large\itshape Shubhankar Deshpande\par}
	\vfill
	Mentor\par
	Dr. Yogesh  \textsc{Wadadekar}
	\par
	Dr. C. H. Ishwar \textsc{Chandra}

	\vfill

\end{titlepage}
\begin{multicols*}{2}
\section{Introduction}
\subsection{Morphological Classes of Radio Galaxies}

Radio galaxies with active nuclei can be distinguished based on their radio luminosity or brightness of their radio emissions in relation to their hosting environment. Some of the basic morphological classifications include point sources, extended sources i.e. sources with extended contours, double radio sources, jets, and lobes.


\subsection{Problems faced with current classification}

Currently Radio astronomers manually classify galaxies based on visual inspection of the images which is a slow procedure, and increases the time to production of scientific results. Further, it introduces uncertainities in the classification procedure, both of which are problems which can potentially be mitigated by using an automated approach.

Contemporary algorithms classify radio sources into at most three different classes. Our aim is to build a robust model capable of handling more than 2 classes.

\section{Objective}

\begin{itemize}
	\item Potentially discovering rare forms of radio sources by classification in different classes.
	\item Reduction in time to generate scientific results by radio astronomers.
	\item Deeper insight into topological representation of radio data during classification.
\end{itemize}

\section{Approach}

\subsection{Source Modelling}
  The first step would be source extraction using the standard technique of gaussian modelling. We propose to do this using the robust PyBDSM pipeline used for fitting gaussian distributions to radio sources. The software contains a plethora of features, from which we would be using a small subset. This would mainly include:

\begin{enumerate}
\item Source extraction using gaussian modelling of radio data.
\item Generation of a catalog file containing details of radio sources (RA, DEC, Size of Gaussian (min, max), etc.)
\end{enumerate}

\subsection{Cutout Generation}
The second step would be to convert the RA(Right Ascension) and DEC (Declination) values generated from the catalog, to their corresponding pixel values in the original image. Based on these pixel values we generate 10*10 px cutouts using as reference the co-ordinates of the center of the radio source. This involves a multistep procedure briefly including:
\begin{enumerate}
\item Reading the FITS image in the form of a matrix
\item Parsing through the generated catalog file and extracting data for each radio source such as RA, DEC, etc.
\item Converting the RA and DEC values from WCS (World Coordinate System) to pixel values.
\item Processing pixel values to account for difference in addressing between FORTRAN and C family of languages.
\item Slicing the image matrix assuming the reference pixel co-ordinates as the center of the source.
\end{enumerate}

Prototype code for section \textit{3.1} and section \textit{3.2} has been written mainly for testing purposes. We used a sample image from the TGSS survey which was then processed using the first two steps of our pipeline to generate 470 cutout images.
More details can be found at: 
https://github.com/NCRA-TIFR/radiogen.

\subsection{Data Preprocessing}

Real world data are incomplete, inconsistent and noisy. Techniques like Data cleaning, integration, transformation, reduction, discretization are required to structure the data uniformly throughout the dataset in the required format.

Some regular techniques are:
\begin{itemize}
\item Mean subtraction 

It involves subtracting the mean across every individual feature in the data, and has the geometric interpretation of centering the cloud of data around the origin along every dimension.

\item Normalization

Normalization refers to normalizing the data dimensions so that they are of approximately the same scale. There are two common ways of achieving this normalization. One is to divide each dimension by its standard deviation, once it has been zero-centered. Another form of this preprocessing normalizes each dimension so that the min and max along the dimension is -1 and 1 respectively. 

\end{itemize}
\begin{figure}[H]
\centering
\includegraphics[scale=0.26]{prepro1.jpeg}
\caption{Mean Subtraction and Normalization}
\end{figure}
\begin{itemize}
\item Dimensionality reduction using Principal Component Analysis

The main linear technique for dimensionality reduction, principal component analysis, performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized. By finding the eigen vector with the highest eigen value we select the Principal component axis with the highest variance and the minimum reconstruction error. Thus we reduce the dimensions by eliminating Principal component axes with the eigen vectors with the least variance thereby not losing much information.
\end{itemize}
\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{pca.png}
\caption{Reduction from 2D to 1D using PCA}
\end{figure}
\subsection{Analytical Approach}

Some approaches that we plan to use: 
\begin{itemize}
\item Scale Invariant Feature Transform (SIFT

Statistical modelling of data to manually extract features. For any object in an image, interesting points on the object can be extracted to provide a "feature description" of the object. This description, extracted from a training image, can then be used to identify the object when attempting to locate the object in a test image containing many other objects. To perform reliable recognition, it is important that the features extracted from the training image be detectable even under changes in image scale, noise and illumination. SIFT is invariant of scaling, transformation and rotation.

\item Edge Detection

1) Reduction of noise using a Gaussian filter
2) Finding gradients along the X and Y directions in the image
3) Suppression of local minima
4) Double Thresholing to determine probable edges
5) Suppression of all weak edges which lack connection
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{edge.jpeg}
\caption{Edge detection}
\end{figure}

\item Image Segmentation

Image Segmentation is the process of partitioning a digital image into multiple segments. The goal of segmentation is to simplify and change the representation of an image into something that is more meaningful and easier to analyze. It is typically used to locate objects and boundaries (lines, curves, etc.) in images. More precisely, image segmentation is the process of assigning a label to every pixel in an image such that pixels with the same label share certain characteristics.
\end{itemize}
\begin{figure}[H]
\centering
\includegraphics[scale=0.1]{segmentation.jpg}
\caption{Segmentation of data into distinct characteristics}
\end{figure}

\subsection{Empirical Approach}

Empirical approach is a way of gaining knowledge by means of direct and indirect observation or experience. In the context of this project, we plan to employ Artificial Neural Networks(ANNs), which are a computational model based on large collection of simple artifical neurons, that learns hierarchical representations of the observational data. Each neural unit is connected with many other units which computes using summation function. Neural networks typically consist of multiple layers in which a signal traverses from the first(input)  to the last (output) layer of neural units.

Convolutional neural networks(CNNs), a particular type of ANN, currently provide the best solutions to many problems in computer vision such as  image segmentation, recognition and classification\cite{alex}.

Convolutional neural networks were designed to use minimal amounts of preprocessing. A typical architecture of a CNN consists of convolutional layers, activation layers and pooling layers. Convolutional layer performs the convolution operation on previous layers, activation layer defines the output of that node given an input or set of inputs, and the pooling layer serves to progressively reduce the spatial size of the representation, to reduce the number of parameters and amount of computation in the network.
    
The initial layers of the CNNs learn simple features in the input data such as straight edges, simple colors, and curves. The deeper layers learn the higher level representation of image and search for complex shapes and structures. Mathematically deeper layers can be thought of as compositions of previous layers. \cite{alex}\cite{Zeiler}\cite{karparthy}. 

\begin{figure}[H]
\centering
\includegraphics[scale=1.5]{cnn.png}
\caption{CNN learning simple features on initial layers for different classes}
\end{figure}

Variants of CNNs have achieved great successes for morphological galaxy classification and prediction on SDSS\cite{Sander}. Sander Dieleman et al. won the Galaxy challenge, an international competition to build the best model for morphology
classication based on annotated images from the Galaxy Zoo project.

\section{Timeline}

\begin{vtimeline}[timeline color=cyan!80!blue, line offset=2pt]
26th April to 11th May & literature survey\endlr
Mid-August to October & Basic prototype model \endlr
October to November & choosing the approaches which work, and implementing them on all data validation of the results\endlr
November to December & Refining the system, cleaning and commenting the code\endlr
\end{vtimeline}

\section{Conclusion}
We would like to thank Dr.Yogesh Wadadekar \footnote{\label{NCRA-TIFR} National Center for Radio Astrophysics - Tata Institute of Fundamental Research}, Dr.C. H. Ishwara Chandra \footnotemark[\ref{NCRA-TIFR}] for their supportive presence during the process of brainstorming potential research ideas. Their constant guidance has been an invaluable source of inspiration for us, and we are eager to continue working with them.

\begin{thebibliography}{9}
\bibitem{alex} 
Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton. 
\textit{ImageNet Classification with Deep Convolutional
Neural Networks}. 

 
\bibitem{Zeiler} 
Matthew D. Zeiler, Rob Fergus.  
\textit{Visualizing and Understanding Convolutional Networks}.  
\newline arXiv print : 1311.2901v3, 2013.

\bibitem{karparthy} 
Andrej Karparthy, Fei-Fei Li, Justin Johnson, Serena Yeung.
\\\texttt{http://cs231n.github.io/convolutional-networks/}

\bibitem{Sander} 
Sander Dieleman, Kyle W. Willett and Joni Dambre  
\textit{Rotation-invariant convolutional neural networks for galaxy morphology prediction}.  
\newline arXiv print : 1503.0707v1, 2015.
\end{thebibliography}
\end{multicols*}
\end{document}
